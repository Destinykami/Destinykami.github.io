---
title: 'CUDA编程基础'
date: 2024/02/15
permalink: /posts/2024/01/cuda-tutorial
excerpt: 'CUDA编程基础，参阅NVIDIA官方文档'
tags:
  - CUDA
  - Machine Learning
---


# TODO
前两章很草率的读过了一遍，主要是一些介绍，干货还是有挺多的，有空的时候整理出来。

# 第三章 Programming Interface
## 3.1 Compilation with NVCC
.cu文件需要使用nvcc来编译，而不是g++.  

## 3.2 Cuda Runtime
Cuda runtime是一个运行库，其中包括了cuda程序运行所需要的各种函数。
### 3.2.2 Device Memory 设备内存
正如异构编程中提到的，一个由主机和设备组成的系统,CUDA编程模型假设每个主机和设备都有自己独立的内存。内核在设备内存之外运行，因此Cuda runtime提供了分配、释放和复制设备内存，以及在主机内存和设备内存之间传输数据的函数。设备内存可以分配为线性内存或CUDA阵列。

CUDA阵列是针对线程簇、线程获取优化的不透明内存布局。

线性内存是在一个统一的地址空间中分配的，这意味着单独分配的实体可以通过指针相互引用。  

> 地址空间大小受到GPU环境的限制，但是我觉得目前可以不用关心这个。表格中的大小至少有40bit，也就是$1024GB$ (计算有错误吗?) ，应该远远超出目前我可用于CUDA编程的显存大小了。  

**线性内存(Liner memory):**
* 使用`cudaMalloc()`分配内存
* 使用`cudaFree()`释放内存
* 使用`cudaMemcpy()`在主机和设备之间传送数据
* `cudaMallocPitch()` and `cudaMalloc3D()`

示例代码： 两个向量相加
```cpp
// Device code
__global__ void VecAdd(float* A, float* B, float* C, int N)
{
    int i = blockDim.x * blockIdx.x + threadIdx.x; //实际上是二维矩阵向一维向量的映射
    if (i < N)
        C[i] = A[i] + B[i];
}

// Host code
int main()
{
    int N = ...;
    size_t size = N * sizeof(float);

    // Allocate input vectors h_A and h_B in host memory
    float* h_A = (float*)malloc(size);
    float* h_B = (float*)malloc(size);
    float* h_C = (float*)malloc(size);

    // Initialize input vectors
    ...

    // Allocate vectors in device memory
    float* d_A;
    cudaMalloc(&d_A, size);
    float* d_B;
    cudaMalloc(&d_B, size);
    float* d_C;
    cudaMalloc(&d_C, size);

    // Copy vectors from host memory to device memory
    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);

    // Invoke kernel
    int threadsPerBlock = 256;
    int blocksPerGrid =
            (N + threadsPerBlock - 1) / threadsPerBlock;
    VecAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);

    // Copy result from device memory to host memory
    // h_C contains the result in host memory
    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);

    // Free device memory
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);

    // Free host memory
    ...
}
```
Linear memory can also be allocated through cudaMallocPitch() and cudaMalloc3D(). These functions are recommended for allocations of 2D or 3D arrays as it makes sure that the allocation is appropriately padded to meet the alignment requirements described in Device Memory Accesses, therefore ensuring best performance when accessing the row addresses or performing copies between 2D arrays and other regions of device memory (using the cudaMemcpy2D() and cudaMemcpy3D() functions). The returned pitch (or stride) must be used to access array elements. The following code sample allocates a width x height 2D array of floating-point values and shows how to loop over the array elements in device code:
```cpp
// Host code
int width = 64, height = 64;
float* devPtr;
size_t pitch;
cudaMallocPitch(&devPtr, &pitch,
                width * sizeof(float), height);
MyKernel<<<100, 512>>>(devPtr, pitch, width, height);

// Device code
__global__ void MyKernel(float* devPtr,
                         size_t pitch, int width, int height)
{
    for (int r = 0; r < height; ++r) {
        float* row = (float*)((char*)devPtr + r * pitch);
        for (int c = 0; c < width; ++c) {
            float element = row[c];
        }
    }
}
```
TODO: 这里面的devPtr\pitch是干什么用的
### 3.2.3 
