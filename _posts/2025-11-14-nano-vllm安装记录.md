---
title: 'nano-vllm安装记录'
date: 2025/11/14
permalink: /posts/2025/11/nano-vllm_install_record/
excerpt: 'nano-vllm安装记录'
tags:
  - AI infra
  - vLLM
---
> 很久没有更新blog了,最近在研究vLLM,所以就记录一下安装过程中遇到的问题。

# nanovllm介绍
nanovllm是对vLLM的一个轻量级实现，主要用于在资源有限的环境中部署大语言模型。
# 安装过程遇到的问题
在安装过程中遇到了Flash-Attention库的问题，导致一直无法正常编译运行，报错 `undefined symbol: .....`

核心原因在于Flash-Attention库需要和Pytorch、Cuda、C++编译器等多个的版本一一对应。

例如`flash_attn-2.7.3+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl` 表示该库适用于`Pytorch 2.6.0,Python 3.11`。

`cxx11abi` 指的是 `C++11` 标准引入的新 `ABI`（也称为`「C++11 ABI」`），用于兼容 `C++11` 及以上标准的特性（如新的容器、线程库等）。
FALSE 表示当前编译环境或程序使用的是「旧 ABI」（即 `C++03` 时代的 ABI），而非 `C++11` 新 `ABI`。

# 安装记录
在下载Flash-Attention前，请确认当前系统的C++编译器版本是否支持`C++11 ABI`。
如果不支持，请下载旧版本的Flash-Attention库。

可以使用以下代码来确认:
```python
import torch
"PyTorch Version:", torch.__version__)
print("Compiled CUDA:", torch.version.cuda)
print("CXX11 ABI:", torch._C._GLIBCXX_USE_CXX11_ABI)

print("\n=== Runtime 信息 ===")
print("CUDA Available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("CUDA Runtime Version:", torch._C._cuda_getRuntimeVersion())
    print("GPU:", torch.cuda.get_device_name(0))
```
首先下载代码
```bash
git clone https://github.com/Destinykami/nano-vllm.git
cd nano-vllm
# 下载编译好的flash-attention库
wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl 
```
建议使用uv来进行包管理。
```bash
uv sync
```
仓库下的pyproject.toml文件修改如下:
```
dependencies = [
    "torch==2.6.0",
    "triton==3.2.0",  # 与 torch2.6 + flash-attn 匹配
    "transformers>=4.51.0",
    "setuptools>=61",
    # 使用你的本地 flash-attn wheel   注意需要下载到当前目录
    "flash-attn @ file:///root/autodl-tmp/nano-vllm/flash_attn-2.7.3+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl",
    "modelscope",
    "xxhash",
]
```
修改之处在于使用编译好的whl文件 而不是从pypi中下载,其次指定了torch版本,新增了modelscope库来解决下载模型的网络问题。

使用 modelscope 来下载模型
```bash
modelscope download --model Qwen/Qwen3-0.6B
```
该命令会自动下载模型到/root/.cache/modelscope/hub/Qwen/Qwen3-0.6B目录下,也可以使用--local-dir参数指定下载目录。

因此在nanovllm的代码下也要修改模型地址为本地的路径，例如example.py内:
```python
    path = os.path.expanduser("/root/.cache/modelscope/hub/models/Qwen/Qwen3-0.6B")
```

最后运行example.py:
```bash
uv run example.py
```
